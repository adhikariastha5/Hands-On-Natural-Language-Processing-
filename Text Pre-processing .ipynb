{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a process that splits an input sequence into so-called tokens.\n",
    "\n",
    "- You can think of token as a useful unit for semantic processing.\n",
    "- Can be a word, sentence , paragraph, etc.\n",
    "\n",
    "Resource: http://text-processing.com/demo/tokenize/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of different tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', \"Paradox's\", 'text,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"This is Paradox's text, isn't it?\"\n",
    "\n",
    "# An exmple of simple whitespace tokenizer\n",
    "\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem: Here \"it\" and \"it?\" are different  tokens with same meaning.\n",
    "- Let's try to split by puntuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Paradox', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem: \"s\" ,\"isn\",\"t\" are not very meaningful\n",
    "\n",
    "- So,we can come up with a set of rules using TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Paradox', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here \"s\" and \"n't\" are more meaningful for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Normalization\n",
    "\n",
    "Text normalization is the process of transforming text into a single canonical form that it\n",
    "might not have had before. Normalizing text before storing or processing it allows for separation of \n",
    "concerns, since input is guaranteed to be consistent before operations are performed on it.\n",
    "\n",
    "For eg. We may want the same token for different forms of the word\n",
    "- wolf,wolves --> wolf\n",
    "- talk,talks --> talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "- A process of removing and replacing suffixes to get the root form of word, which is called the <b>stem.<b>\n",
    "- Usually refers to heuristics that chop off suffixes.\n",
    " \n",
    " \n",
    "### Stemming Example:\n",
    "\n",
    "Porter's Stemmer\n",
    "\n",
    "- 5 heuristics phases of word reductions, applied sequentially \n",
    "- Example of phase 1 rules:\n",
    "\n",
    "    <b>Rule<b>               <b>Example <b>\n",
    "    \n",
    "    SSES --> SS              caresses --> caress\n",
    "    IES  --> I               ponies   --> poni\n",
    "    SS   --> SS              caress   --> caress\n",
    "    S    -->                 cats     --> cat\n",
    "    \n",
    "- nltk.stem.PorterStemmer\n",
    "- Examples:\n",
    "   feet--> feet      cats--> cat\n",
    "   wolves --> wolv   talked-->talk\n",
    "   \n",
    "- Problem: Fails on irregualr form, produces non-words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feet cat wolv talk'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"feet cats wolves talked\"\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\" \".join(stemmer.stem(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "- Usually refers to doing things properly with the use of a vocabulary and morphological analysis.\n",
    "- Returns the base or dictionary form of a word, which is known as the <b>lemma.<b>\n",
    "    \n",
    "### Lemmatization Example:\n",
    "\n",
    "WordNet Lemmatizer\n",
    "\n",
    "- Uses the WordNet Database to lookup lemmas\n",
    "- nltk.stem.WordNetLemmatizer\n",
    "- Examples:\n",
    "    -feet--> foot    cats-->cat\n",
    "    -wolves--> wolf  talked--> talked\n",
    "- Problems: Not all forms are reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foot cat wolf talked'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(stemmer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "txt = nltk.tokenize.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "txt=[word for word in txt if word not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway:  We need to try stemming or lemmatization and choose best for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
